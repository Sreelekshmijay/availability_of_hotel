# -*- coding: utf-8 -*-
"""availability_of_hotel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wh4KbVhtr9lWLa2GOf5_Z8KHzHW-iNsY
"""

import sys
!{sys.executable}- m pip install pandas seaborn

import pandas as pd
pd.set_option("display.max_columns", 101)

#data = pd.read_csv("train.csv")
rows = pd.read_csv('train.csv', skiprows=[0], header = None)
#print(rows)
df= pd.DataFrame(rows)
print(df)
# skip header line
rows = rows.dropna(axis=1, how='all')
# drop columns that only have NaNs

rows.columns = pd.read_csv('train.csv', nrows=0).columns

rows.head()

#analyzing the data numerical values
rows.describe()

#Checking data types for each column
rows.dtypes

#Checking whether any data is null
rows.isnull().sum(axis=0)

#filling nulls with 0
rows.reviews_per_month = rows.reviews_per_month.fillna(0)
rows= pd.get_dummies(rows, drop_first = True)
rows

#remove non relevant data
train_data = rows.drop(['id', 'latitude', 'longitude', 'owner_id'], axis=1)

#load test data
test_data = pd.read_csv('test.csv')
test_data.head()

#cleaning test data
test_data.isnull().sum(axis=0)

test_data.reviews_per_month = test_data.reviews_per_month.fillna(0)
test_data = pd.get_dummies(test_data,drop_first = True)
test_data_feature = test_data.drop(['id', 'latitude', 'longitude', 'owner_id'], axis=1)
test_data_feature

#checking correlation
import seaborn as sns
import matplotlib.pyplot as plt
corr_df = train_data.corr()
corr_df

ax = plt.axes()
sns.heatmap(corr_df, ax = ax)
ax.set_title('Correlation between features')
plt.show()

corr_df_viz = corr_df
corr_df_viz['feature'] = corr_df_viz.index
plt.figure(figsize = (10,6))

#create bargraph
sns.barplot(x= 'feature', y = "yearly_availability", data = corr_df_viz, order= corr_df_viz.sort_values('yearly_availability', ascending = False).feature)

#set labels
plt.xlabel("Feature", size=15)
plt.ylabel("Correlation Between Yearly availability", size= 15)
plt.title("Top 20 Features", size = 20)
plt.tight_layout()
plt.xticks(rotation = 80)

#create model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, confusion_matrix ,classification_report
import numpy as np

#checkig distribution of yearly availability
f, ax = plt.subplots(figsize= (7,5))
sns.countplot(x='yearly_availability', data=train_data)
plt.title('# Availability vs Non Availability')
plt.xlabel('Class (1==Availability)')

#split data to train and test
np.random.seed(42)
X= train_data.drop(['yearly_availability'], axis = 1)
y= train_data['yearly_availability']
X_train, X_test, y_train, y_test = train_test_split(X, y)

#using logistic regression for binary classification
scaler = StandardScaler()
lr = LogisticRegression(solver = 'lbfgs')
model1 = Pipeline([('standardize', scaler), ('log_read', lr)])
model1.fit(X_train, y_train)
Pipeline(memory= None, steps =[('standardize', StandardScaler(copy = True, with_mean = True, with_std = True)),
                               ('log_reg',
                 LogisticRegression(C=1.0, class_weight=None, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=100,
                                    multi_class='warn', n_jobs=None,
                                    penalty='l2', random_state=None,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)

# Accuracy for Train Split
y_train_h = model1.predict(X_train)
y_train_h_probs = model1.predict_proba(X_train)[:,1]

train_accuracy = accuracy_score(y_train, y_train_h)*100

print('Confusion matrix:\n', confusion_matrix(y_train, y_train_h))

print('Training accuracy: %.4f %%' % train_accuracy)

# Accuracy for Test Split
y_test_h = model1.predict(X_test)
y_test_h_probs = model1.predict_proba(X_test)[:,1]

test_accuracy = accuracy_score(y_test, y_test_h)*100

print('Confusion matrix:\n', confusion_matrix(y_test, y_test_h))

print('Testing accuracy: %.4f %%' % test_accuracy)





print(classification_report(y_test, y_test_h, digits=5))

# Combining prediction results to IDs.
pred_list = list(model1.predict(test_data_feature))
pred_id = list(test_data['id'])
submission_df = pd.DataFrame(list(zip(pred_list,pred_id )), columns = ['id','yearly_availability'])
submission_df

submission_df.to_csv('submissions.csv',index=False)

"""# New section

# New section
"""